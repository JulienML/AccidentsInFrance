{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import sklearn.discriminant_analysis as DA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this preliminary work, we will visualize the data and draw some first results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering\n",
    "\n",
    "As we have different csv files containing the data, the first thing to do is to merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we import the four csv files that we are going to use\n",
    "df1 = pd.read_csv(\"./data/caracteristics.csv\", encoding='latin-1')\n",
    "df2 = pd.read_csv(\"./data/places.csv\", encoding=\"latin-1\")\n",
    "df3 = pd.read_csv(\"./data/users.csv\", encoding=\"latin-1\")\n",
    "df4 = pd.read_csv(\"./data/vehicles.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Then, we merge all the dataframes together using the column 'Num_Acc' that they have in common\n",
    "df1 = pd.merge(df1, df2, how=\"outer\", on=\"Num_Acc\")\n",
    "df2 = pd.merge(df3, df4, how='outer', on=\"Num_Acc\")\n",
    "\n",
    "initial_data = pd.merge(df1, df2, how='outer', on=\"Num_Acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we filter it by selecting only the columns we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we select the columns that we want to keep and we rename it appropriately\n",
    "columns = ['an', 'mois', 'catu', 'grav', 'sexe', 'an_nais', 'trajet', 'secu', 'lum', 'atm', 'catr', 'surf']\n",
    "\n",
    "data = initial_data[columns]\n",
    "data.columns = ['Year', 'Month', 'User category', 'Severity', 'Sex', 'Year of birth', 'Trip purpose', 'Securiy', 'Luminosity', 'Weather', 'Type of road', 'Road surface']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "First, we plot the number of accidents that occurred each month from January 2005 to December 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe 'time' with only the information about the time of the accidents\n",
    "time = data[['Year', 'Month']]\n",
    "\n",
    "# We add a column 'Year-Month' containing the month and the year of the accident\n",
    "time['Year-Month'] = time['Year'].astype('str') + \" - \" + time['Month'].astype('str')\n",
    "\n",
    "# We plot first the number of accidents per month\n",
    "plt.figure()\n",
    "\n",
    "sns.countplot(data=time, x='Year-Month')\n",
    "\n",
    "plt.xlabel(\"Month and year\")\n",
    "plt.ylabel(\"Number of accidents in France\")\n",
    "plt.xticks(ticks=[i for i in range(0, 144, 12)], labels=[\"01/\"+ str(i) for i in range(2005,2017)], rotation=45)\n",
    "plt.title(\"Number of accidents per month in France between 2005 and 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the number of accidents in France tend to increase along time. To verify this hypothesis, let’s plot the number of accidents through years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the number of accidents per year\n",
    "plt.figure()\n",
    "\n",
    "sns.countplot(data=time, x='Year')\n",
    "\n",
    "plt.xticks(ticks=plt.xticks()[0], labels=range(2005,2017))\n",
    "plt.ylabel(\"Number of accidents in France\")\n",
    "plt.title(\"Number of accidents per year in France between 2005 and 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradoxically, we observe that the number of accidents per year decreases: from above 350000 in 2005 to approximately 250000 in 2016. If we look again at the first plot, we can observe that during the first years, the number of accidents through months is more regular than in the last years (where there are a lot of peaks). Then, maybe this contradiction is because in 2016 there are months during which the number of accidents is way bigger than during the others, so we have the impression that in the first plot the numbers of accidents increases whereas in reality it decreases through years.\n",
    "\n",
    "Let’s now look at the number of accidents for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the number of accidents per month, no matter what the year is\n",
    "plt.figure()\n",
    "\n",
    "sns.countplot(data=time, x='Month')\n",
    "\n",
    "plt.xticks(ticks=plt.xticks()[0], labels=['January', 'February','March','April','May','June','July','August','September','October','November',\"December\"], rotation=45)\n",
    "\n",
    "plt.ylabel(\"Number of accidents in France\")\n",
    "plt.title(\"Number of accidents in France for each month (between 2005 and 2016)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have thought about several hypothesis such as the fact that there will be a lot of accidents during the summer holidays (July and August) because more people are using the car to go in vacation, or the fact that in winter there are much more cars on the roads so it leads to more accidents, but we can see that these hypotheses are false. We observe that the peaks of accidents are reached in June, July, and October while the lowest numbers of accidents are reached in February and August."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of the columns that we want to keep that we rename appropriately\n",
    "columns = ['an', 'mois', 'catu', 'grav', 'sexe', 'an_nais', 'trajet', 'secu', 'lum', 'atm', 'catr', 'surf']\n",
    "\n",
    "data = initial_data[columns]\n",
    "data.columns = ['Year', 'Month', 'User category', 'Severity', 'Sex', 'Year of birth', 'Trip purpose', 'Security', 'Luminosity', 'Weather', 'Type of road', 'Road surface']\n",
    "\n",
    "# Calculate the mean of each feature (column)\n",
    "means = data.mean(axis=1)\n",
    "\n",
    "# Normalization of the data - Calculation of the standard deviation of each feature (column)\n",
    "datanorm = StandardScaler().fit_transform(data)\n",
    "datanorm = pd.DataFrame(datanorm, columns=['Year', 'Month', 'User category', 'Severity', 'Sex', 'Year of birth', 'Trip purpose', 'Security', 'Luminosity', 'Weather', 'Type of road', 'Road surface'])\n",
    "datanorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(datanorm.corr().round(2), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of the data\n",
    "data = data.fillna(data.mean())\n",
    "X = data.to_numpy()\n",
    "Xstd = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "Xpca = pca.fit_transform(Xstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccircle = []\n",
    "eucl_dist = []\n",
    "for i,j in enumerate(X .T):\n",
    "    corr1 = np.corrcoef(j,Xpca[:,0])[0,1]\n",
    "    corr2 = np.corrcoef(j,Xpca[:,1])[0,1]\n",
    "    ccircle.append((corr1, corr2))\n",
    "    eucl_dist.append(np.sqrt(corr1**2 + corr2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    fig, axs = plt.subplots(figsize=(6, 6))\n",
    "    for i,j in enumerate(eucl_dist):\n",
    "        arrow_col = plt.cm.cividis((eucl_dist[i] - np.array(eucl_dist).min())/\\\n",
    "                                (np.array(eucl_dist).max() - np.array(eucl_dist).min()) )\n",
    "        axs.arrow(0,0, # Arrows start at the origin of the graphic\n",
    "                 ccircle[i][0],  #0 for PC1\n",
    "                 ccircle[i][1],  #1 for PC2\n",
    "                 lw = 2, # establishment of line width\n",
    "                 length_includes_head=True, \n",
    "                 color = arrow_col,\n",
    "                 fc = arrow_col,\n",
    "                 head_width=0.05,\n",
    "                 head_length=0.05)\n",
    "        # The labels of the arrows in relatively reasonable size\n",
    "        axs.text(ccircle[i][0]/2,ccircle[i][1]/2, data.columns[i], fontsize=7)\n",
    "    # Draw the unit circle\n",
    "    circle = Circle((0, 0), 1, facecolor='none', edgecolor='k', linewidth=1, alpha=0.5)\n",
    "    axs.add_patch(circle)\n",
    "    axs.set_xlabel(\"PCA 1\", fontsize=10)\n",
    "    axs.set_ylabel(\"PCA 2\", fontsize=10)\n",
    "\n",
    "    axs.set_xlabel(\"PCA 1\")\n",
    "    axs.set_ylabel(\"PCA 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Clusters\n",
    "\n",
    "In this part, we will first display maps representing the accidents, then we'll perform various clusterings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns that we want to keep and we rename it appropriately\n",
    "columns = ['an', 'mois', 'grav', 'lat', 'long']\n",
    "\n",
    "data = initial_data[columns]\n",
    "data.columns = ['Year', 'Month', 'Severity', 'Latitude', 'Longitude']\n",
    "\n",
    "# Delete all rows for which we have no information about location\n",
    "data.dropna(axis=0, subset=['Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "# Rearrange the 'Severity' numbers in ascending order\n",
    "data['Severity'].replace({2: 4, 4: 2}, inplace=True)\n",
    "\n",
    "# Create a dictionary with a description for each level of severity\n",
    "severity_description = {\n",
    "    1: \"Unscathed\",\n",
    "    2: \"Light injury\",\n",
    "    3: \"Hospitalized wounded\",\n",
    "    4: \"Killed\"\n",
    "}\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace wrong characters with '0' and convert it to double\n",
    "data['Longitude'] = data['Longitude'].replace('-', 0).astype('double')\n",
    "data['Latitude'] = data['Latitude'].replace('-', 0).astype('double')\n",
    "\n",
    "# Keep only the accidents located in mainland france\n",
    "data = data.loc[(data['Longitude'] < 3*10**6) & (data['Latitude'] > 3*10**6) & (data['Latitude'] < 5.2*10**6)]\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first display in simple map of the all the accidents between 2005 and 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.scatterplot(data=data, x='Longitude', y='Latitude', s=0.1)\n",
    "plt.title(\"Accidents in France between 2005 and 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that a lot of accidents occur in the biggest cities: Paris, Marseille, Lyon...\n",
    "Now, let's display a different map for each severity coefficient. This way, we will see if the accidents' location is different according to the severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "for i in range(1, 5):\n",
    "    plt.subplot(1, 4, i)\n",
    "    sns.scatterplot(data=data.loc[data['Severity']==i], x='Longitude', y='Latitude', s=0.1)\n",
    "    plt.axis(False)\n",
    "    plt.title(f\"Accidents in France with severity: '{severity_description[i]}' ({i})\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the location is not linked to the severity: accidents both severe and non-severe accidents occur in cities and on roads.\n",
    "\n",
    "Let's try to display a map of the accidents for each year between 2005 and 2016. This way, we will see if the accidents' locations change over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "for i in range(1, 13):\n",
    "    plt.subplot(3, 4, i)\n",
    "    sns.scatterplot(data=data.loc[data['Year']==4+i], x='Longitude', y='Latitude', s=0.1)\n",
    "    plt.axis(False)\n",
    "    plt.title(f\"Accidents in France in {2004 + i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, we can think that there are more accidents in the cities over years, but we have to be careful about the fact that we have not the same amount of data per year. For example, the 2009's accidents have no information about location, so we have no map. Therefore, the fact that there are more accidents in the cities is maybe due to the fact that we have more data in the most recent years.\n",
    "\n",
    "In any case, we can note from the previous two blocks of code that the accidents are grouped around France's biggest cities. We will try to confirm this hypothesis thanks to clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our hypothesis, we will first use KMeans clustering method to divide the accidents into 15 clusters. In addition to that, we will plot on the map the 22 French metropolises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "\n",
    "# Perform KMeans clusterization\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto').fit(data[['Longitude', 'Latitude']])\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Create a figure and plot all the accidents with different colors according to the different clusters\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.scatterplot(data=data, x=\"Longitude\", y=\"Latitude\", hue=labels, palette=sns.color_palette(\"bright\", n_clusters), s=0.1)\n",
    "\n",
    "# Define the longitude and latitude of the 22 french metropolises\n",
    "metropolises_coordinates = {\n",
    "    \"Paris\" : (2.33*10**5, 48.83*10**5),\n",
    "    \"Lille\" : (3.05*10**5, 50.63*10**5),\n",
    "    \"Rouen\" : (1.09*10**5, 49.45*10**5),\n",
    "    \"Metz\" : (6.18*10**5, 49.12*10**5),\n",
    "    \"Nancy\" : (6.18*10**5, 48.69*10**5),\n",
    "    \"Strasbourg\" : (7.75*10**5, 48.58*10**5),\n",
    "    \"Brest\" : (-4.49*10**5, 48.39*10**5),\n",
    "    \"Rennes\" : (-1.69*10**5, 48.11*10**5),\n",
    "    \"Nantes\" : (-1.56*10**5, 47.22*10**5),\n",
    "    \"Tours\" : (0.69*10**5, 47.39*10**5),\n",
    "    \"Orléans\" : (1.92*10**5, 47.90*10**5),\n",
    "    \"Dijon\"  : (5.03*10**5, 47.32*10**5),\n",
    "    \"Clermont-Ferrand\" : (3.08*10**5, 45.78*10**5),\n",
    "    \"Lyon\" : (4.83*10**5, 45.77*10**5),\n",
    "    \"Saint-Etienne\" : (4.39*10**5, 45.44*10**5),\n",
    "    \"Grenoble\" : (5.73*10**5, 45.19*10**5),\n",
    "    \"Bordeaux\" : (-0.58*10**5, 44.84*10**5),\n",
    "    \"Toulouse\" : (1.45*10**5, 43.60*10**5),\n",
    "    \"Montpellier\" : (3.89*10**5, 43.60*10**5),\n",
    "    \"Marseille-Aix-en-Provence\" : (5.38*10**5, 43.30*10**5),\n",
    "    \"Toulon\" : (5.94*10**5, 43.12*10**5),\n",
    "    \"Nice\" : (7.27*10**5, 43.70*10**5)\n",
    "}\n",
    "\n",
    "# Add the metropolises to the plot\n",
    "for city in metropolises_coordinates:\n",
    "    plt.scatter(metropolises_coordinates[city][0], metropolises_coordinates[city][1], marker='x', color='black')\n",
    "    plt.text(metropolises_coordinates[city][0], metropolises_coordinates[city][1] + 0.05*10**5, s=city, ha='center', va='bottom')\n",
    "\n",
    "# Define the plot's legend\n",
    "legend = []\n",
    "for i, color in enumerate(sns.color_palette(\"bright\", n_clusters)):\n",
    "    legend.append(mpatches.Patch(color=color, label=f\"Cluster {i + 1}\"))\n",
    "\n",
    "cross = Line2D([0], [0], label='Metropolises', marker='x', markeredgecolor='black', linestyle='')\n",
    "legend.append(cross)\n",
    "\n",
    "# Add the legend and show the result\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "plt.title(f\"Accidents in France between 2005 and 2016, divided in {n_clusters} clusters (KMeans method)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we clearly see that the accidents are grouped around the different metropolises. However, as we didn't know what number of clusters was optimal, we chose 15 arbitrary. Indeed, the KMeans method needs the number of clusters as input to work. To be more rigorous, we can choose a method that determines by itself the optimal number of clusters, such as the DBSCAN method.\n",
    "\n",
    "The DBSCAN uses different parameters than the KMeans method. The two main parameters are epsilon (`eps`) and the minimum samples (`min_samples`). The epsilon corresponds to the maximum distance between two points for them to be considered part of the same cluster. The minimum samples value is the minimum number of points that are needed to form a cluster. As we want the clusters to be cities, we fix it to 300 points.\n",
    "\n",
    "To find the optimal epsilon value, we use the method described in [this article](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf). First, we use the NearestNeighbors class to calculate the average distance between each point in the dataset and its nearest neighbors. Then, we sort it in ascending order, and we plot it. The ideal value will be equal to the distance value at the “crook of the elbow”, or the point of maximum curvature. This is what we do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between each point and its nearest neighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=4)\n",
    "neighbors_fit = neighbors.fit(data[['Longitude', 'Latitude']])\n",
    "distances, indices = neighbors_fit.kneighbors(data[['Longitude', 'Latitude']])\n",
    "\n",
    "# Sort it in ascending order\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "\n",
    "# Plot the distances\n",
    "plt.figure()\n",
    "plt.plot(distances, label='Distances')\n",
    "plt.title(\"Nearest Neighbors Distances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distances\n",
    "plt.figure()\n",
    "plt.plot(distances, label='Distances')\n",
    "\n",
    "# We find the point of maximum curvature and calculate the tangent\n",
    "point_index = np.argmin(np.abs(np.gradient(distances) - 1))\n",
    "tangent_line = np.arange(len(distances)) - (point_index - distances[point_index])\n",
    "\n",
    "# We plot the point and the tangent\n",
    "plt.scatter(point_index, distances[point_index], color='red', label='Point with a 45-degree slope')\n",
    "plt.plot(tangent_line, label=f'Tangent', linestyle='--')\n",
    "\n",
    "# Zoom on the crook of the elbow\n",
    "plt.legend()\n",
    "plt.axis([900000, 950000, 0, 5000])\n",
    "plt.title(\"Zoom on the 'crook of the elbow' of Nearest Neighbors Distances\")\n",
    "plt.show()\n",
    "\n",
    "# Print the espilon value found\n",
    "print(f\"Optimal Epsilon value: {distances[point_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal epsilon value, we can apply the DBSCAN clustering method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBBSCAN clusterization method\n",
    "db = DBSCAN(eps=2250, min_samples=300).fit(data[['Longitude', 'Latitude']])\n",
    "labels = db.labels_\n",
    "\n",
    "# Count and print the number of clusters\n",
    "n_clusters = len(set(labels)) - 1\n",
    "print(f\"{n_clusters} clusters found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Create a custom palette where each cluster have a bright color, and unclassified points are gray\n",
    "custom_palette = {}\n",
    "palette = sns.color_palette('bright', n_clusters)\n",
    "\n",
    "k=0\n",
    "for cluster in set(labels):\n",
    "    if cluster != -1:\n",
    "        custom_palette[cluster] = palette[k]\n",
    "        k+=1\n",
    "    else:\n",
    "        custom_palette[cluster] = (0.5, 0.5, 0.5)\n",
    "\n",
    "# Create a column 'Cluster' with the corresponding cluster for each point\n",
    "data['Cluster'] = labels\n",
    "\n",
    "# Plot all the accidents with different colors using the custom palette\n",
    "scatter = sns.scatterplot(data=data, x=\"Longitude\", y=\"Latitude\", hue='Cluster', palette=custom_palette, s=0.1, legend=None)\n",
    "\n",
    "# Plot a marker on the centroids of each cluster\n",
    "centroids = data.groupby('Cluster')[['Longitude', 'Latitude']].mean()\n",
    "plt.scatter(centroids['Longitude'], centroids['Latitude'], marker='x', color='black', label=\"Clusters' centroids\")\n",
    "\n",
    "# Add noisy points to the figure's legend\n",
    "legend, _ = scatter.get_legend_handles_labels()\n",
    "point = Line2D([0], [0], label='Noisy points', marker='o', markerfacecolor='gray', markeredgecolor='gray', linestyle='')\n",
    "legend.append(point)\n",
    "\n",
    "# Show the legend and display the figure\n",
    "plt.legend(handles=legend, loc='upper right')\n",
    "plt.title(f\"Accidents in France between 2005 and 2016, divided in {n_clusters} clusters (DBSCAN method)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map confirms our hypothesis: the clusters' centroids are located on the biggest French cities so this is where most accidents occur, particularly around Paris and on the mediterranean coast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Predictions\n",
    "\n",
    "In this part, we will compare several classification models to determine which one can predict the best the severity of an accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correlation matrix in the first part, we retain the features that are the most correlated to the accident's severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns that we want to keep and we rename it appropriately\n",
    "columns = ['catu', 'sexe', 'an_nais', 'secu', 'lum', 'atm', 'catr', 'surf', 'grav']\n",
    "\n",
    "data = initial_data[columns]\n",
    "data.columns = ['User category', 'Sex', 'Year of birth', 'Security', 'Luminosity', 'Weather', 'Type of road', 'Road surface', 'Severity']\n",
    "\n",
    "# Delete all rows for which one or more column is empty (NA)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Rearrange the 'Severity' numbers in ascending order\n",
    "data['Severity'].replace({2: 4, 4: 2}, inplace=True)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the different classofication method\n",
    "\n",
    "We will now divide the table into two parts: one for training the models and another for testing them. As we have a very large dataset, we choose the following distribution: 60% for training and 40% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='Severity').copy()\n",
    "y = data[['Severity']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.40, random_state=1234)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is separated, we test 4 different classification models: Logit, NBayes, LDA and QDA. Then, we plot the accuracy metrics of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names of the classifications method\n",
    "names = [\"Logit\", \"NBayes\", \"LDA\", \"QDA\"]\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(C=1e5),\n",
    "               GaussianNB(), \n",
    "               DA.LinearDiscriminantAnalysis(),\n",
    "               DA.QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "# Write the head line of the table describing the accuracy of each method\n",
    "print('Name   Accuracy  Precision  Recall  F1_score\\n'+44*'-')\n",
    "\n",
    "# Create a new figure for the histogram\n",
    "plt.figure()\n",
    "\n",
    "# Loop through all different classfication methods\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    # Predict the diagnosis using the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy metrics\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Print the classifier's name and its accuracy score\n",
    "    print('{:6} {:7.3f} {:9.3f} {:9.3f} {:8.3f}'.format(name, accuracy, precision, recall, f1_score))\n",
    "\n",
    "    # Create a bar in the histogram for each accuracy metrics\n",
    "    plt.bar(names.index(name) - 0.2, precision, color='blue', width=0.2)\n",
    "    plt.bar(names.index(name), recall, color='g', width=0.2)\n",
    "    plt.bar(names.index(name) + 0.2, f1_score, color='r', width=0.2)\n",
    "\n",
    "# Write the classification methods' names on the x axis\n",
    "plt.xticks(range(len(names)), names)\n",
    "\n",
    "# Add a xlabel, ylabel and title to the plot\n",
    "plt.xlabel(\"Classification methods\")\n",
    "plt.ylabel(\"Accuracy metrics\")\n",
    "plt.title(\"Accuracy metrics by classification methods\")\n",
    "\n",
    "# Add the legend to the plot\n",
    "plt.legend([\"precision\", \"recall\", \"f1_score\"])\n",
    "\n",
    "# Increase the size of the plot so that the legend does not hide the metrics of the latest classification method.\n",
    "plt.ylim(top=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we observe that it is difficult to predict the severity of an accident because the correlations between the severity and the features we chose are too weak. However, the best classification model is Linear Discriminant Analysis (LDA), with a success rate of around 50%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".A4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
